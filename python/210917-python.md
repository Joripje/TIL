# 210917 TIL

## 오늘 배운 것

### 데이터 크롤링

- 웹 크롤링: 웹 서비스 내 정보를 수집
    - 정보를 가져오고자 하는 url 정의
    - url 정보로 requests 패키지를 사용, 정보 요청
    - text 정보를 BeutifulSoup를 사용해 html로 변환
    - html 파일에서 원하는 데이터를 셀렉트

- `requests.get(url)` url 정보 요청
- `BuetifulSoup( requests로 얻은 데이터.text, 'html.parser')` text를 html로 변환
- `html.select()` 원하는 데이터 검색
    - tag, class, id 등을 이용해 검색 가능
    - ex) `html.select('span')` tag가 span인 데이터 검색  
`html.select('span.news)` tag가 span class가 news인 데이터  
`html.select('span#1234')` tag가 span, id가 1234인 데이터
    - 셀렉트한 데이터는 리스트 형태
- 반복문을 사용해 여러 개의 셀렉트 데이터 출력

- requests.codes.ok
    - 100 서비스 예정
    - 200 성공
    - 300 사이트 이전
    - 400 유저의 잘못된 요청
    - 500 서버 문제

- robots.txt: 데이터 수집 범위, 요청 딜레이 등 주의
